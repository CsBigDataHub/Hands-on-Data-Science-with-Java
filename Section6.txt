Section 6.1 Code
#There are different ways to perform the hadoop installation in windows
#One :Install Virtual Machine and install Ubuntu OS, and you can then perfom the installation starting with creating hadoop user command
#Second : Install in the server eg I used a droplet to perform the hadoop installation. For this to work, you need to ssh to the server ip and login. Before starting, get all the updates and then you can continue with the installation  from creating the hadoop user .
#Only for those connecting to the droplet

ssh root@IP

#Check all updates
apt-get update

#Continue as Normally

#Create a Hadoop User
adduser hadoopuser

#Add User to sudo group

usermod -aG sudo hadoopuser
sudo addgroup hadoop
usermod -aG hadoop hadoopuser

#Switch to hadoop User:
 sudo su hadoopuser

#Switch to Home directory and start working
cd /usr/local

#Installing Java
sudo wget https://download.java.net/openjdk/jdk10/ri/jdk-10_linux-x64_bin_ri.tar.gz

tar xzvf jdk-10_linux-x64_bin_ri.tar.gz 
sudo mkdir /usr/lib/jvm/
sudo mkdir /usr/lib/jvm/java-10-openjdk-amd64
sudo mv jdk-10 /usr/lib/jvm/java-10-openjdk-amd64

sudo update-alternatives --install /usr/bin/java java /usr/lib/jvm/java-10-openjdk-amd64/jdk-10/bin/java 1
sudo update-alternatives --install /usr/bin/javac javac /usr/lib/jvm/java-10-openjdk-amd64/jdk-10/bin/javac 1
sudo update-alternatives --install /usr/bin/jps jps /usr/lib/jvm/java-10-openjdk-amd64/jdk-10/bin/jps 1

sudo update-alternatives --config java
sudo update-alternatives --config javac

java --version
javac --version


#adding ssh keys
ssh-keygen -t rsa -P ''
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

#Dowloading Hadoop 
wget https://www-eu.apache.org/dist/hadoop/common/hadoop-3.0.2/hadoop-3.0.2.tar.gz
tar xzvf hadoop-3.0.2.tar.gz 
sudo mv hadoop-3.0.2 /usr/local/hadoop

#Configurations:
#HADOOP VARIABLES START
export JAVA_HOME=/usr/lib/jvm/java-10-openjdk-amd64/jdk-10
export HADOOP_INSTALL=/usr/local/hadoop
export PATH=$PATH:$HADOOP_INSTALL/bin
export PATH=$PATH:$HADOOP_INSTALL/sbin
export HADOOP_MAPRED_HOME=$HADOOP_INSTALL
export HADOOP_COMMON_HOME=$HADOOP_INSTALL
export HADOOP_HDFS_HOME=$HADOOP_INSTALL
export YARN_HOME=$HADOOP_INSTALL
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_INSTALL/lib/native"

#New Created
source ~/.bashrc
nano /usr/local/hadoop/etc/hadoop/hadoop-env.sh
export JAVA_HOME=/usr/lib/jvm/java-10-openjdk-amd64/jdk-10

#Editing /usr/local/hadoop/etc/hadoop/core-site.xml
nano /usr/local/hadoop/etc/hadoop/core-site.xml
<property>
   <name>fs.default.name</name>
   <value>hdfs://localhost:9000</value>
</property>

#Editing /usr/local/hadoop/etc/hadoop/yarn-site.xml
nano /usr/local/hadoop/etc/hadoop/yarn-site.xml
<property>
   <name>yarn.nodemanager.aux-services</name>
   <value>mapreduce_shuffle</value>
</property>
<property>
   <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
   <value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>

#Creating and Editing /usr/local/hadoop/etc/hadoop/mapred-site.xml
nano /usr/local/hadoop/etc/hadoop/mapred-site.xml
<property>
   <name>mapreduce.framework.name</name>
   <value>yarn</value>
</property>

#Editing /usr/local/hadoop/etc/hadoop/hdfs-site.xml, creating Master and Slave
sudo mkdir -p /usr/local/hadoop_store/hdfs/namenode
sudo mkdir -p /usr/local/hadoop_store/hdfs/datanode

#Open hdfs and edit
nano /usr/local/hadoop/etc/hadoop/hdfs-site.xml

<property>
   <name>dfs.replication</name>
   <value>1</value>
 </property>
 <property>
   <name>dfs.namenode.name.dir</name>
   <value>file:/usr/local/hadoop_store/hdfs/namenode</value>
 </property>
 <property>
   <name>dfs.datanode.data.dir</name>
   <value>file:/usr/local/hadoop_store/hdfs/datanode</value>
 </property>

#Make Hadoopuser the owner of hadoop_store
sudo chown hadoopuser:hadoopuser -R /usr/local/hadoop_store/

#Format the Hadoop File System
hdfs namenode -format

#Start Hadoop
start-dfs.sh

#Start Yarn
start-yarn.sh

#Verify Hadoop is Running
jps


#Possible trouble shooting Ideas:
#idea1: If datanode does not start
Stop dfs
Open hdfs-site.xml
Remove data.dir and name.dir properties from the hdfs-site.xml
Format the namenode
Start dfs again

#Idea 2 : If Yarn does not start











